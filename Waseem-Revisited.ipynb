{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Waseem Revisited</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we reproduce the experiment conducted in <a href=https://www.aclweb.org/anthology/N16-2013.pdf>Waseem 2016</a> to detect hate speech in Tweets, but with newly retrieved and annotated test data. Our hypothesis is that training on the same data and testing on new data will result in lower accuracy than on the old data. We reproduced the original method for retrieving Tweets using the following (particularly prone) search terms: “MKR”, “asian drive”, “feminazi”, “immigrant”, “n\\*\\*\\*r”, “sjw”, “WomenAgainstFeminism”, “blameonenotall”, “islam terrorism”, “notallmen”, “victimcard”, “victim card”, “arab terror”, “gamergate”, “jsil”, “racecard”, and “race card”. We also used stratified sampling to get a representative distribution in the new test data for Tweets from the first three months of 2021. Even though we performed many aspects of the experiment in the same way, the results were significantly different. First, consider the distribution of terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Retrieving Tweets</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team used Indiana University's OSoMe service to retrieve Tweets between 01/03/2021 and 04/03/2021 for a three-month window using the keywords and keyphrases above as search queries. We took the following steps to get the data file for sampling:\n",
    "<ul>\n",
    "    <li>Retrieve .gzip files from OSoMe.</li>\n",
    "    <li>Extract .gzip files to retrieve JSON files.</li>\n",
    "    <li>Merge all JSON files.</li>\n",
    "    <ul>\n",
    "        <li>Only include Tweet IDs and extended Tweet content</li>\n",
    "        <li>Skip truncated tweets (which applied to many retweets)</li></ul>\n",
    "    <li>Remove Tweets with duplicate content using pandas.</li>\n",
    "    <li>Remove non-English Tweets using langdetect.</li>\n",
    "    <li>Write the results to a CSV file.</li></ul>\n",
    "<br>\n",
    "This yielded the included tweets.csv file. The total number of Tweets is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n",
      "Done.\n",
      "\n",
      "Number of Tweets:\n",
      "----------------\n",
      "100082\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"Reading CSV...\")\n",
    "df = pd.read_csv(\"tweets/tweets.csv\", encoding='utf8')\n",
    "print(\"Done.\")\n",
    "print()\n",
    "print(\"Number of Tweets:\")\n",
    "print(\"----------------\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the proportion of each keyword or keyphrase with the following two functions, counting only one term per Tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Case-sensitive strings.\n",
    "kws = ['MKR',\n",
    "       'feminazi',\n",
    "       'immigrant',\n",
    "       'nigger',\n",
    "       'sjw',\n",
    "       'WomenAgainstFeminism',\n",
    "       'blameonenotall',\n",
    "       'notallmen',\n",
    "       'victimcard',\n",
    "       'gamergate',\n",
    "       'jsil',\n",
    "       'racecard',\n",
    "       'race card',\n",
    "       'asian drive',\n",
    "       'islam terrorism',\n",
    "       'arab terror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(data, kws, col):\n",
    "\n",
    "    kw_list = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        contained_kws = []\n",
    "        for kw in kws:\n",
    "            if kw in row[col]:\n",
    "                contained_kws.append(kw)\n",
    "        if len(contained_kws) >= 1:\n",
    "            # Count only the first string.\n",
    "            kw_list.append(contained_kws[0])\n",
    "        else:\n",
    "            # Count the absence of any term as None \n",
    "            # (e.g., strings may be in the wrong case)\n",
    "            kw_list.append(None)\n",
    "\n",
    "    new_col = np.array(kw_list)\n",
    "\n",
    "    data['Keyword'] = new_col\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_proportions(data, kws, col):\n",
    "\n",
    "    kw_counts = {}\n",
    "    kw_percentages = {}\n",
    "    kw_proportions = {}\n",
    "    data_size = len(data)\n",
    "    \n",
    "    for kw in kws:\n",
    "        count = data[col].str.contains(kw, na=False).sum()\n",
    "        kw_counts[kw] = count\n",
    "    \n",
    "    for key in kw_counts:\n",
    "        kw_percentages[key] = \"%.2f\" % float(kw_counts[key]*100/data_size)\n",
    "        kw_proportions[key] = float(kw_counts[key]*100/data_size)\n",
    "\n",
    "    # Create a table with the results.\n",
    "    table = pd.DataFrame(kw_counts.items())\n",
    "    table[2] = table[0].map(kw_proportions)\n",
    "    table[3] = table[0].map(kw_percentages)\n",
    "    table.columns = ['Keyword','Count','Proportion','%']\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tweet Content</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1366176476563914756</td>\n",
       "      <td>Why is this nigger driving the car?</td>\n",
       "      <td>nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1366177438753988613</td>\n",
       "      <td>y’all have no clue the RAGE that would consume...</td>\n",
       "      <td>nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1366177499143634945</td>\n",
       "      <td>One white gyal call me a nigger at a party she...</td>\n",
       "      <td>nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1366177543200641032</td>\n",
       "      <td>The world's biggest promotion has begun\\n\\n$AR...</td>\n",
       "      <td>MKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1366179153372979209</td>\n",
       "      <td>@sham786364 @bobpockrass @NASCARONFOX The real...</td>\n",
       "      <td>immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100077</th>\n",
       "      <td>1366132457767129095</td>\n",
       "      <td>@michaellooby2 @jackh1092 @Josh_HW Yeah they w...</td>\n",
       "      <td>race card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100078</th>\n",
       "      <td>1366134724389986308</td>\n",
       "      <td>@bmorepolo5 @MidwestBake @flockxfans You're pu...</td>\n",
       "      <td>race card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100079</th>\n",
       "      <td>1366134770095251461</td>\n",
       "      <td>@kapitalkeyz1 @rosaluxemburgs Lol nah. Legally...</td>\n",
       "      <td>race card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100080</th>\n",
       "      <td>1366149052530843650</td>\n",
       "      <td>It's the race card all over again.</td>\n",
       "      <td>race card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100081</th>\n",
       "      <td>1366161830062796810</td>\n",
       "      <td>@marceelias There is nothing wrong with what h...</td>\n",
       "      <td>race card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76602 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Tweet ID  \\\n",
       "0       1366176476563914756   \n",
       "2       1366177438753988613   \n",
       "3       1366177499143634945   \n",
       "4       1366177543200641032   \n",
       "6       1366179153372979209   \n",
       "...                     ...   \n",
       "100077  1366132457767129095   \n",
       "100078  1366134724389986308   \n",
       "100079  1366134770095251461   \n",
       "100080  1366149052530843650   \n",
       "100081  1366161830062796810   \n",
       "\n",
       "                                            Tweet Content    Keyword  \n",
       "0                     Why is this nigger driving the car?     nigger  \n",
       "2       y’all have no clue the RAGE that would consume...     nigger  \n",
       "3       One white gyal call me a nigger at a party she...     nigger  \n",
       "4       The world's biggest promotion has begun\\n\\n$AR...        MKR  \n",
       "6       @sham786364 @bobpockrass @NASCARONFOX The real...  immigrant  \n",
       "...                                                   ...        ...  \n",
       "100077  @michaellooby2 @jackh1092 @Josh_HW Yeah they w...  race card  \n",
       "100078  @bmorepolo5 @MidwestBake @flockxfans You're pu...  race card  \n",
       "100079  @kapitalkeyz1 @rosaluxemburgs Lol nah. Legally...  race card  \n",
       "100080                 It's the race card all over again.  race card  \n",
       "100081  @marceelias There is nothing wrong with what h...  race card  \n",
       "\n",
       "[76602 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = create_label(df, kws, \"Tweet Content\")\n",
    "labeled_data = labeled_data.dropna(subset=['Keyword']) # Get rid of NaN values.\n",
    "labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Count</th>\n",
       "      <th>Proportion</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MKR</td>\n",
       "      <td>4273</td>\n",
       "      <td>4.269499</td>\n",
       "      <td>4.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feminazi</td>\n",
       "      <td>776</td>\n",
       "      <td>0.775364</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>immigrant</td>\n",
       "      <td>45905</td>\n",
       "      <td>45.867389</td>\n",
       "      <td>45.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nigger</td>\n",
       "      <td>8053</td>\n",
       "      <td>8.046402</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sjw</td>\n",
       "      <td>3383</td>\n",
       "      <td>3.380228</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WomenAgainstFeminism</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blameonenotall</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>notallmen</td>\n",
       "      <td>178</td>\n",
       "      <td>0.177854</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>victimcard</td>\n",
       "      <td>34</td>\n",
       "      <td>0.033972</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gamergate</td>\n",
       "      <td>1413</td>\n",
       "      <td>1.411842</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jsil</td>\n",
       "      <td>17</td>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>racecard</td>\n",
       "      <td>206</td>\n",
       "      <td>0.205831</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>race card</td>\n",
       "      <td>12338</td>\n",
       "      <td>12.327891</td>\n",
       "      <td>12.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>asian drive</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005995</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>islam terrorism</td>\n",
       "      <td>7</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>arab terror</td>\n",
       "      <td>13</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Keyword  Count  Proportion      %\n",
       "0                    MKR   4273    4.269499   4.27\n",
       "1               feminazi    776    0.775364   0.78\n",
       "2              immigrant  45905   45.867389  45.87\n",
       "3                 nigger   8053    8.046402   8.05\n",
       "4                    sjw   3383    3.380228   3.38\n",
       "5   WomenAgainstFeminism      0    0.000000   0.00\n",
       "6         blameonenotall      0    0.000000   0.00\n",
       "7              notallmen    178    0.177854   0.18\n",
       "8             victimcard     34    0.033972   0.03\n",
       "9              gamergate   1413    1.411842   1.41\n",
       "10                  jsil     17    0.016986   0.02\n",
       "11              racecard    206    0.205831   0.21\n",
       "12             race card  12338   12.327891  12.33\n",
       "13           asian drive      6    0.005995   0.01\n",
       "14       islam terrorism      7    0.006994   0.01\n",
       "15           arab terror     13    0.012989   0.01"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_proportions(df, kws, \"Keyword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Sampling Tweets</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a representative sample of the data obtained above, we performed stratified sampling using the Python library, pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tweet Content</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1346938089051189248</td>\n",
       "      <td>She definitely wanted to say nigger 😂</td>\n",
       "      <td>nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1371527461968736263</td>\n",
       "      <td>This is the fate we all have been faced with i...</td>\n",
       "      <td>immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1352853571079966725</td>\n",
       "      <td>‘Leftists are weak’ seems to be a popular phra...</td>\n",
       "      <td>sjw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1373644115246735363</td>\n",
       "      <td>@achillistyy @anti_functional @boardhopping im...</td>\n",
       "      <td>immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1352152292234035201</td>\n",
       "      <td>Looking for an excellent binary exchange? 👀 Us...</td>\n",
       "      <td>MKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1369301209065140229</td>\n",
       "      <td>Still not familiar with the immigrant agenda i...</td>\n",
       "      <td>immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1346727273345511431</td>\n",
       "      <td>Me )in some kids playing hide and go seek some...</td>\n",
       "      <td>nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1373858172197163009</td>\n",
       "      <td>@chrisbritt01 Ur race card has expired.</td>\n",
       "      <td>race card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1346587076616200194</td>\n",
       "      <td>I totally get that this was a joke, and yet I ...</td>\n",
       "      <td>immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1368671709981794305</td>\n",
       "      <td>@zarahsultana Oooh are we about to play a race...</td>\n",
       "      <td>race card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tweet ID                                      Tweet Content  \\\n",
       "0    1346938089051189248              She definitely wanted to say nigger 😂   \n",
       "1    1371527461968736263  This is the fate we all have been faced with i...   \n",
       "2    1352853571079966725  ‘Leftists are weak’ seems to be a popular phra...   \n",
       "3    1373644115246735363  @achillistyy @anti_functional @boardhopping im...   \n",
       "4    1352152292234035201  Looking for an excellent binary exchange? 👀 Us...   \n",
       "..                   ...                                                ...   \n",
       "596  1369301209065140229  Still not familiar with the immigrant agenda i...   \n",
       "597  1346727273345511431  Me )in some kids playing hide and go seek some...   \n",
       "598  1373858172197163009            @chrisbritt01 Ur race card has expired.   \n",
       "599  1346587076616200194  I totally get that this was a joke, and yet I ...   \n",
       "600  1368671709981794305  @zarahsultana Oooh are we about to play a race...   \n",
       "\n",
       "       Keyword  \n",
       "0       nigger  \n",
       "1    immigrant  \n",
       "2          sjw  \n",
       "3    immigrant  \n",
       "4          MKR  \n",
       "..         ...  \n",
       "596  immigrant  \n",
       "597     nigger  \n",
       "598  race card  \n",
       "599  immigrant  \n",
       "600  race card  \n",
       "\n",
       "[601 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 601 # Sample size. Set higher than needed for rounding errors.\n",
    "\n",
    "groups = labeled_data.groupby('Keyword', group_keys=False)\n",
    "proportional_groups = groups.apply(lambda x: x.sample(int(np.rint((N*len(x)/(len(labeled_data)))))))\n",
    "sample = proportional_groups.sample(frac=1)\n",
    "sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, now we can verify that the proportions are close to the desired proportions. It is difficult to get the *exact* proportions given the significantly smaller size of the sample data. <b>Note:</b> this is not the exact sample found in the /data directory, and was not used for the actual experiment. This is just a demonstration of the method used. Regardless, the proportions are roughly the same for both samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Count</th>\n",
       "      <th>Proportion</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MKR</td>\n",
       "      <td>34</td>\n",
       "      <td>5.657238</td>\n",
       "      <td>5.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feminazi</td>\n",
       "      <td>6</td>\n",
       "      <td>0.998336</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>immigrant</td>\n",
       "      <td>360</td>\n",
       "      <td>59.900166</td>\n",
       "      <td>59.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nigger</td>\n",
       "      <td>63</td>\n",
       "      <td>10.482529</td>\n",
       "      <td>10.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sjw</td>\n",
       "      <td>27</td>\n",
       "      <td>4.492512</td>\n",
       "      <td>4.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WomenAgainstFeminism</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blameonenotall</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>notallmen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166389</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>victimcard</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gamergate</td>\n",
       "      <td>11</td>\n",
       "      <td>1.830283</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jsil</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>racecard</td>\n",
       "      <td>2</td>\n",
       "      <td>0.332779</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>race card</td>\n",
       "      <td>97</td>\n",
       "      <td>16.139767</td>\n",
       "      <td>16.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>asian drive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>islam terrorism</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>arab terror</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Keyword  Count  Proportion      %\n",
       "0                    MKR     34    5.657238   5.66\n",
       "1               feminazi      6    0.998336   1.00\n",
       "2              immigrant    360   59.900166  59.90\n",
       "3                 nigger     63   10.482529  10.48\n",
       "4                    sjw     27    4.492512   4.49\n",
       "5   WomenAgainstFeminism      0    0.000000   0.00\n",
       "6         blameonenotall      0    0.000000   0.00\n",
       "7              notallmen      1    0.166389   0.17\n",
       "8             victimcard      0    0.000000   0.00\n",
       "9              gamergate     11    1.830283   1.83\n",
       "10                  jsil      0    0.000000   0.00\n",
       "11              racecard      2    0.332779   0.33\n",
       "12             race card     97   16.139767  16.14\n",
       "13           asian drive      0    0.000000   0.00\n",
       "14       islam terrorism      0    0.000000   0.00\n",
       "15           arab terror      0    0.000000   0.00"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_proportions(sample, kws, \"Keyword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Comparing with the Original Data</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Forthcoming.* I found it more difficult to get accurate proportions of the Waseem test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there were some aspects of the experiment that were not performed the same, as will be described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Methodological Differences</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Data Retrieval</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall project was comprised of three teams of graduate students with three members each. We conducted data retrieval independently of one another, combining the final samples.  The other teams had slightly different or smaller windows and a slightly different distribution of keywords. The Waseem data was collected *over the course* of two months, with no specified timeframe for the Tweets themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Annotators</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original annotations were done by two annotators with outside expert verification. In our case, each of the three teams retrieved a unique set of 600 Tweets for a total of 1800 Tweets. Then, each member of each team was solely responsible for annotating a subset of 200 Tweets with no inter-rater reliability metric or expert verification. The resulting label distribution for our test data was 297 Tweets labeled for hate speech and 1490 labeled for none, giving a ratio of 1:5. Compare this to the original test data distribution of 496 Tweets labeled for hate speech and 1076 labeled for none, giving a drastically different ratio of 1:2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Definitions</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our three teams did not confer about a governing definition of hate speech, although we did discuss it as a class. Our team relied on the definition given in <a href=https://dl.acm.org/doi/abs/10.1145/3232676>Fortuna 2018</a>:\n",
    "<br>\n",
    "<br>\n",
    "<blockquote>\n",
    "Hate speech is language that attacks or diminishes, that incites violence or hate against groups, based on specific characteristics such as physical appearance, religion, descent, national or ethnic origin, sexual orientation, gender identity or other, and it can occur with different linguistic styles, even in subtle forms or when humour is used.\n",
    "</blockquote>\n",
    "<br>\n",
    "This definition, along with its associated criteria, differs from that given by Waseem, thus creating some differences in annotations. For example, the following Tweet was tagged as hate speech in the original test data:\n",
    "<br>\n",
    "<br>\n",
    "<blockquote>\n",
    "Islam is worse than the Nazi party ever was.\n",
    "</blockquote>\n",
    "<br>\n",
    "Whereas Fortuna states that <q>speak[ing] badly about countries or religions (e.g., France, Portugal, Catholicism, Islam) is allowed in general, but discrimination is not allowed based on these categories.</q> Thus our labels on similar Tweets were not tagged as hate speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Libraries Used</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal preprocessing is performed. Only URLs are removed. Other preprocessing steps, such as removing stop words, are saved as features for GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    cleaned_data = []\n",
    "    for line in data:\n",
    "        cleaned_data.append(p.clean(line))\n",
    "    return cleaned_data\n",
    "\n",
    "def get_data(path):\n",
    "    with open(path, mode='r', encoding = 'utf-8') as f:\n",
    "        data = list(f)\n",
    "    return preprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Definitions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Logistic Regression</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(train_X, train_y, test_X, test_y):\n",
    "    \n",
    "    #Define the logistic regression (LR) pipeline.\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LogisticRegression(random_state=0))\n",
    "    ])\n",
    "    \n",
    "    #Define the optional parameters to be tried with LR.\n",
    "    params = {\n",
    "        'vect__stop_words': [None, 'english'],\n",
    "        'vect__ngram_range': [(1,1),(2,2),(1,2),(2,3)],\n",
    "        'clf__penalty': ('l1','l2','elasticnet'),\n",
    "        'clf__C': (1,2,3),\n",
    "        'clf__fit_intercept': (True, False),\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "    \n",
    "    #Try optional parameters with GridSearchCV\n",
    "    print(\"Running logistic regression...\")\n",
    "    gs_clf = GridSearchCV(text_clf, params, cv=5, n_jobs=-1)\n",
    "    gs_clf.fit(train_X, train_y)\n",
    "    preds = gs_clf.predict(test_X)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    #Get the best parameters.\n",
    "    best_params = gs_clf.best_params_\n",
    "    print()\n",
    "    print(\"Best parameters:\")\n",
    "    print(\"---------------\")\n",
    "    for key in best_params.keys():\n",
    "        print(key + ': ' + str(best_params[key]))\n",
    "    print()\n",
    "    \n",
    "    #Get the classification report.\n",
    "    report = classification_report(test_y, preds, digits=6)\n",
    "    print(\"Classification report:\")\n",
    "    print(\"---------------------\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Support Vector Machine</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(train_X, train_y, test_X, test_y):\n",
    "    #Define the support vector machine (SVM) pipeline.\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svm.SVC(gamma='scale'))\n",
    "    ])\n",
    "    \n",
    "    #Define the optional parameters to be tried with SVM.\n",
    "    params = {\n",
    "        'vect__stop_words': [None, 'english'],\n",
    "        'vect__ngram_range':[(1,1),(2,2),(1,2),(2,3)],\n",
    "        'clf__C':(1, 2, 3),\n",
    "        'clf__kernel': ('linear','sigmoid','rbf'),\n",
    "        'clf__shrinking':(True, False),\n",
    "        'tfidf__use_idf':(True, False)\n",
    "    }\n",
    "    \n",
    "    #Try optional parameters with GridSearchCV\n",
    "    print(\"Running support vector classifier...\")\n",
    "    gs_clf = GridSearchCV(text_clf, params, cv=5, n_jobs=-1)\n",
    "    gs_clf.fit(train_X, train_y)\n",
    "    preds = gs_clf.predict(test_X)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    #Get the best parameters.\n",
    "    best_params = gs_clf.best_params_\n",
    "    print()\n",
    "    print(\"Best parameters:\")\n",
    "    print(\"---------------\")\n",
    "    for key in best_params.keys():\n",
    "        print(key + ': ' + str(best_params[key]))\n",
    "    print()\n",
    "    \n",
    "    #Get the classification report.\n",
    "    report = classification_report(test_y, preds, digits=6)\n",
    "    print(\"Classification report:\")\n",
    "    print(\"---------------------\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Random Forest</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(train_X, train_y, test_X, test_y):\n",
    "    #Define the random forest (RF) pipeline.\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', rf())\n",
    "    ])\n",
    "    \n",
    "    #Define the optional parameters to be tried with RF.\n",
    "    params = {\n",
    "        'vect__stop_words': [None, 'english'],\n",
    "        'vect__ngram_range':[(1,1),(2,2),(1,2),(2,3)],\n",
    "        'clf__n_estimators': (10, 30, 50, 100),\n",
    "        'clf__criterion':('gini','entropy'),\n",
    "        'clf__max_features':('sqrt','log2'),\n",
    "        'tfidf__use_idf':(True, False)\n",
    "    }\n",
    "    \n",
    "    #Try optional parameters with GridSearchCV\n",
    "    print(\"Running random forest...\")\n",
    "    gs_clf = GridSearchCV(text_clf, params, cv=5, n_jobs=-1)\n",
    "    gs_clf.fit(train_X, train_y)\n",
    "    preds = gs_clf.predict(test_X)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    #Get the best parameters.\n",
    "    best_params = gs_clf.best_params_\n",
    "    print()\n",
    "    print(\"Best parameters:\")\n",
    "    print(\"---------------\")\n",
    "    for key in best_params.keys():\n",
    "        print(key + ': ' + str(best_params[key]))\n",
    "    print()\n",
    "    \n",
    "    #Get the classification report.\n",
    "    report = classification_report(test_y, preds, digits=6)\n",
    "    print(\"Classification report:\")\n",
    "    print(\"---------------------\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Naive Bayes</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(train_X, train_y, test_X, test_y):\n",
    "    #Define the naive bayes (NB) pipeline.\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    #Define the optional parameters to be tried with NB.\n",
    "    params = {\n",
    "        'vect__stop_words': [None, 'english'],\n",
    "        'vect__ngram_range':[(1,1),(2,2),(1,2),(2,3)],\n",
    "        'clf__alpha': (0.000001,1),\n",
    "        'clf__fit_prior':(True, False),\n",
    "        'tfidf__use_idf':(True, False)\n",
    "    }\n",
    "    \n",
    "    #Try optional parameters with GridSearchCV\n",
    "    print(\"Running naive bayes...\")\n",
    "    gs_clf = GridSearchCV(text_clf, params, cv=5, n_jobs=-1)\n",
    "    gs_clf.fit(train_X, train_y)\n",
    "    preds = gs_clf.predict(test_X)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    #Get the best parameters.\n",
    "    best_params = gs_clf.best_params_\n",
    "    print()\n",
    "    print(\"Best parameters:\")\n",
    "    print(\"---------------\")\n",
    "    for key in best_params.keys():\n",
    "        print(key + ': ' + str(best_params[key]))\n",
    "    print()\n",
    "    \n",
    "    #Get the classification report.\n",
    "    report = classification_report(test_y, preds, digits=6)\n",
    "    print(\"Classification report:\")\n",
    "    print(\"---------------------\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Original Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relevant data. This is the original training and testing data used in Waseem 2016. 'X' corresponds to Tweet contents while 'y' corresponds to their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading original data...\")\n",
    "train_X = get_data('data/waseemtrain.txt')\n",
    "test_X = get_data('data/waseemtest.txt')\n",
    "train_y = get_data('data/waseemtrainGold.txt')\n",
    "test_y = get_data('data/waseemtestGold.txt')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Original Model and Results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running logistic regression...\n",
      "Done.\n",
      "\n",
      "Best parameters:\n",
      "---------------\n",
      "clf__C: 1\n",
      "clf__fit_intercept: True\n",
      "clf__penalty: l2\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (2, 3)\n",
      "vect__stop_words: None\n",
      "\n",
      "Classification report:\n",
      "---------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.876471  0.300403  0.447447       496\n",
      "           2   0.752496  0.980483  0.851493      1076\n",
      "\n",
      "    accuracy                       0.765903      1572\n",
      "   macro avg   0.814484  0.640443  0.649470      1572\n",
      "weighted avg   0.791613  0.765903  0.724008      1572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>New Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same training data, test on newly obtained and annotated Tweets to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new test data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading new test data...\")\n",
    "test_new_X = get_data('data/waseem_new_test.txt')\n",
    "test_new_y = get_data('data/waseem_new_testGold.txt')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>New Models and Results</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Retrying Logistic Regression</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running logistic regression...\n",
      "Done.\n",
      "\n",
      "Best parameters:\n",
      "---------------\n",
      "clf__C: 1\n",
      "clf__fit_intercept: True\n",
      "clf__penalty: l2\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (2, 3)\n",
      "vect__stop_words: None\n",
      "\n",
      "Classification report:\n",
      "---------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.416667  0.033670  0.062305       297\n",
      "           2   0.837209  0.990604  0.907470      1490\n",
      "\n",
      "    accuracy                       0.831561      1787\n",
      "   macro avg   0.626938  0.512137  0.484888      1787\n",
      "weighted avg   0.767315  0.831561  0.767003      1787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR(train_X, train_y, test_new_X, test_new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Trying Support Vector Machine</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running support vector classifier...\n",
      "Done.\n",
      "\n",
      "Best parameters:\n",
      "---------------\n",
      "clf__C: 1\n",
      "clf__kernel: rbf\n",
      "clf__shrinking: True\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (2, 2)\n",
      "vect__stop_words: None\n",
      "\n",
      "Classification report:\n",
      "---------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.325000  0.043771  0.077151       297\n",
      "           2   0.837436  0.981879  0.903923      1490\n",
      "\n",
      "    accuracy                       0.825965      1787\n",
      "   macro avg   0.581218  0.512825  0.490537      1787\n",
      "weighted avg   0.752269  0.825965  0.766514      1787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM(train_X, train_y, test_new_X, test_new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Trying Random Forest</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running random forest...\n",
      "Done.\n",
      "\n",
      "Best parameters:\n",
      "---------------\n",
      "clf__criterion: entropy\n",
      "clf__max_features: log2\n",
      "clf__n_estimators: 50\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (2, 3)\n",
      "vect__stop_words: None\n",
      "\n",
      "Classification report:\n",
      "---------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.333333  0.006734  0.013201       297\n",
      "           2   0.834363  0.997315  0.908591      1490\n",
      "\n",
      "    accuracy                       0.832680      1787\n",
      "   macro avg   0.583848  0.502025  0.460896      1787\n",
      "weighted avg   0.751091  0.832680  0.759777      1787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF(train_X, train_y, test_new_X, test_new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Trying Naive Bayes</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running naive bayes...\n",
      "Done.\n",
      "\n",
      "Best parameters:\n",
      "---------------\n",
      "clf__alpha: 1\n",
      "clf__fit_prior: True\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (2, 3)\n",
      "vect__stop_words: None\n",
      "\n",
      "Classification report:\n",
      "---------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.437500  0.023569  0.044728       297\n",
      "           2   0.836251  0.993960  0.908310      1490\n",
      "\n",
      "    accuracy                       0.832680      1787\n",
      "   macro avg   0.636875  0.508764  0.476519      1787\n",
      "weighted avg   0.769978  0.832680  0.764783      1787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB(train_X, train_y, test_new_X, test_new_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
